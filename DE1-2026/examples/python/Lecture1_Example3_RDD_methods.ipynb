{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73139c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTERNODE_PRIVATE_IP=\"192.168.2.30\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7401c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up Spark Session / Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076905ba-5c2e-4a45-a9b5-44910d92e011",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/10 07:09:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(f\"spark://{MASTERNODE_PRIVATE_IP}:7077\") \\\n",
    "        .appName(\"Lecture1_Example3_RDD_function_examples\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 8)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68761c",
   "metadata": {},
   "source": [
    "<img align=left src=\"images/pyspark-rdd.svg\" width=750 height=750 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8011d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1af528-7269-4598-8481-cd6e489b4d8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Using getNumPartitions\n",
    "<img align=left src=\"images/pyspark-getNumPartitions.svg\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d1b02e-b279-4021-afc6-de8532716d8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getNumPartitions\n",
    "rdd = spark_context.parallelize(range(100_000_000), 16)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743fc35",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb32fb-0905-40e1-a46e-1f93459c1c03",
   "metadata": {},
   "source": [
    "## 2. Using map & mapValues:\n",
    "<img align=left src=\"images/pyspark-map.svg\" width=475 height=500/>\n",
    "<img align=right src=\"images/pyspark-mapValues.svg\" width=475 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d310c7-ae2e-424c-984a-b97322f35d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of map: ['a:apple-banana-lemon', 'b:grapes']\n",
      "Result of mapValues (example 1): [('a', 3), ('b', 1)]\n",
      "Result of mapValues (example 2): [('a', 'apple-banana-lemon'), ('b', 'grapes')]\n"
     ]
    }
   ],
   "source": [
    "x = spark_context.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): return x[0] + \":\" + \"-\".join(x[1])\n",
    "\n",
    "# map\n",
    "print(\"Result of map:\", x.map(f).collect())\n",
    "\n",
    "# mapValues - example 1\n",
    "def f(x): return len(x)\n",
    "print(\"Result of mapValues (example 1):\", x.mapValues(f).collect())\n",
    "\n",
    "# mapValues - example 2\n",
    "def f(x): return \"-\".join(x)\n",
    "print(\"Result of mapValues (example 2):\", x.mapValues(f).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af4d44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a38e0-5094-4c31-9d50-e1e401f0897b",
   "metadata": {},
   "source": [
    "## 3. Using flatMap & flatMapValues:\n",
    "<img align=left src=\"images/pyspark-flatMap.svg\" width=475 height=475 />\n",
    "<img align=left src=\"images/pyspark-flatMapValues.svg\" width=475 height=475 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c53c81-b498-421c-b4e9-0c5baf3812c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of flatMap: PythonRDD[6] at RDD at PythonRDD.scala:58\n",
      "Result of flatMapValues: [('a', 3), ('a', 4), ('a', 5), ('b', 7), ('b', 8)]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "x = spark_context.parallelize([1, 2, 3, 4, 5, 6])\n",
    "print(\"Result of flatMap:\", x.flatMap(lambda x: (x, 100*x, x**2)))\n",
    "\n",
    "# flatMapValues\n",
    "x = spark_context.parallelize([(\"a\", [1, 2, 3]), (\"b\", [5, 6])])\n",
    "print(\"Result of flatMapValues:\", x.flatMapValues(lambda x: [i+2 for i in x]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba0106",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b861d52-d6bd-4f21-ac0c-37094d7fa707",
   "metadata": {},
   "source": [
    "## 4. Using mapPartitions & mapPartitionsWithIndex:\n",
    "<img align=left src=\"images/pyspark-mapPartition.svg\" width=475 height=475/>\n",
    "<img align=left src=\"images/pyspark-mapPartitionWithIndex.svg\" width=475 height=475/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846aef9d-17d9-4827-9ff6-d5c360ea62f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19531246875000], [58593746875000], [97656246875000], [136718746875000], [175781246875000], [214843746875000], [253906246875000], [292968746875000], [332031246875000], [371093746875000], [410156246875000], [449218746875000], [488281246875000], [527343746875000], [566406246875000], [605468746875000]]\n",
      "[[(0, 19531246875000)], [(1, 58593746875000)], [(2, 97656246875000)], [(3, 136718746875000)], [(4, 175781246875000)], [(5, 214843746875000)], [(6, 253906246875000)], [(7, 292968746875000)], [(8, 332031246875000)], [(9, 371093746875000)], [(10, 410156246875000)], [(11, 449218746875000)], [(12, 488281246875000)], [(13, 527343746875000)], [(14, 566406246875000)], [(15, 605468746875000)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# rdd = spark_context.parallelize(range(100_000_000), 16)\n",
    "\n",
    "# mapPartitions\n",
    "def f(iterator): yield sum(iterator)\n",
    "\n",
    "rdd_2 = rdd.mapPartitions(f)\n",
    "print(rdd_2.glom().collect())  # glom() flattens elements on the same partition\n",
    "\n",
    "\n",
    "# mapPartitionsWithIndex\n",
    "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
    "\n",
    "rdd_2 = rdd.mapPartitionsWithIndex(f)\n",
    "print(rdd_2.glom().collect())  # glom() flattens elements on the same partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7380a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c549aa-aff1-4919-9165-cc5aa984f980",
   "metadata": {},
   "source": [
    "## 5. Using filter:\n",
    "<img align=left src=\"images/pyspark-filter.svg\" width=500 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3b59fb-450e-4b59-8f1e-7f75827bf978",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                            (8 + 8) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of filter: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# filter\n",
    "rdd_2 = rdd.filter(lambda x: x%2 == 1 and x < 20)\n",
    "print(\"Result of filter:\", rdd_2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67682b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddec765-008c-46d6-9956-0fc4d6044c1c",
   "metadata": {},
   "source": [
    "## 6. Using distinct:\n",
    "<img align=left src=\"images/pyspark-distinct.svg\" width=500 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed7e563d-7ade-4435-b8ff-ddb3b00cb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD: ['A', 'A', 'B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>       (14 + 2) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct RDD: ['B', 'A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "x = spark_context.parallelize([\"A\",\"A\",\"B\"])\n",
    "y = x.distinct()\n",
    "print(\"Original RDD:\", x.collect())\n",
    "print(\"Distinct RDD:\", y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24261b5-a166-4e9f-9d6b-3eb3b615fa2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ab7e5-e1ea-4e02-8dfd-5bb03c934d92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
